<!DOCTYPE html>
<html lang="en">

  <head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!-- Meta, title, CSS, favicons, etc. -->
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="">
<meta name="keywords" content="">
<meta name="author" content="Rui Liu">

<title>Rui Liu's Homepage</title>

<!-- Bootstrap core CSS -->
<link href="./css/bootstrap.min.css" rel="stylesheet">

<link href="./css/custom.css" rel="stylesheet">
<link href="./css/syntax.css" rel="stylesheet">

<!--[if lt IE 9]><script src="../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->


<!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
<!--[if lt IE 9]>
  <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
  <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->


  <style type="text/css"></style>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-9372397-3', 'auto');
  ga('send', 'pageview');

</script>
</head>

  <body data-twttr-rendered="true">
      <header class="navbar navbar-static-top bs-docs-nav custom_navbar navbar-fixed-top" id="top" role="banner">
    <div class="color_wrapper"></div>
  <div class="container">
    <div class="navbar-header">
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target=".bs-navbar-collapse">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav class="collapse navbar-collapse bs-navbar-collapse">
      <ul class="nav navbar-nav navbar-right">
        
		<li><a href="#biography">Biography</a></li>
        <li><a href="#publication">Publications</a></li>
		<li><a href="#project">Projects</a></li>
		<li><a href="#talks">Talks</a></li>
		<li><a href="#activities">Activities</a></li>
		<li><a href="#award">Awards</a></li>
		<li><a href="#award">Resource</a></li>
      </ul>
    </nav>
  </div>
</header>
<div class="nav-space"></div>
  <div class="page-titile-wraper aboutme-wraper">
  <div class="container">
    <div class="row">
      <div class="col-md-3"><img src="assets/RuiLiu.jpg" style= "margin-top:5px" class="img-responsive img-circle"></div>
      <div class="col-md-9" style="line-height:5px"> 
        <h1 id="about"><b>Rui Liu (刘 瑞) </b></h1> 
		
		
		<h3><b>Ph.D.</b></h3>
	
	   <!-- <h4><a href="./index_chn.html"> 中文主页 </a></h4> !-->
	  <h4><a href="https://ccs.imu.edu.cn/info/1152/4634.htm"> 中文主页 </a></h4>

		<h4>
		
		<a href="http://www.mglip.com/">National and Local Joint Engineering Research Center of Mongolian Intelligent Information Processing</a></h4>
		
		<h4><a href="https://www.imu.edu.cn/">Inner Mongolia University</a>, China</h4>
		<br>
		
		<h4>Office: 503 Room, School of Computer science, Inner Mongolia University, China 010021</h4>
		
		<h4>Mobile: +86 16647162610</h4>


		<h4>Email: <a href="mailto: iurui_imu@163.com">liurui_imu@163.com</a>    
		</h4>
	

		<br>
			<!--
                <h4>

		<a href="">Prospective Students:<br>

I'm recruiting active graduate students. Please contact me if you are a self-motivated graduate student with strong programming skills and interests in software analysis and mining software repository.
               </a>
              </h4>
              !-->
        



      </div>
    </div>
  </div>
</div>






<div class="container">
	<div class="row">
		<div class="col-md-12" role="main">

			<br> <br><h2><a href="./index_S2Group.html"> Speech understanding and Speech generation (S2) Research Group<br> (语音理解与生成研究组)  </a></h2> 



		<h2 id="biography" class="page-header">Biography</h2>
			<div class="row">
				<div class="col-md-12">

				 <p class="text-justify">
				 	Rui Liu is currently a Professor in National and Local Joint Engineering Research Center of Mongolian Intelligent Information Processing, Inner Mongolia University.
					Rui Liu received Ph.D degree from Inner Mongolia University, China in 2020 and Bachelor degree in Taiyuan University of Technology, ShanXi, China in 2014. From 2020 to 2022, he worked as a Research Fellow at the Department of Electrical and Computer Engineering, National University of Singapore, Singapore, working with<a href="http://www.colips.org/~eleliha/"> Prof. Haizhou Li.</a> He was the recipient of the "Best Paper Award" at the 2021 International Conference on Asian Language Processing.
					<br>He has published more than 20 papers in top-tier NLP/ML/AI conferences and journals, including IEEE/ACM-TASLP, Neural Networks, ICASSP, COLING, INTERSPEECH, etc. Dr. Liu serves as the reviewer for many major referred journal and conference papers. His research interests broadly lie in audio, speech and natural language processing, which include expressive Text-to-Speech (TTS), expressive voice conversion, speech emotion recognition, prosody structure prediction, grapheme-to-phoneme conversion (G2P), syntax parsing et. al. <br>
					<br>
					<h4><b>Experiences</b></h4>
					<ol class="paper-list" id="grants">
						<li>Jan 2022-Present, Professor  <br> <a href="http://www.mglip.com/">National and Local Joint Engineering Research Center of Mongolian Intelligent Information Processing</a>, Inner Mongolia University, China.</li>

					<li>Aug 2020-Jan 2022, Research Fellow (Advisor: Prof. <a href="http://www.colips.org/~eleliha/">Haizhou Li</a>)<br> <a href="https://www.eng.nus.edu.sg/ece/hlt/">HLT Lab</a>, National University of Singapore, Singapore.</li>
					<li>Aug 2019-Aug 2020, Visiting Ph.D. Student (Advisor: Prof. <a href="http://www.colips.org/~eleliha/">Haizhou Li</a>)<br>
					<a href="https://www.eng.nus.edu.sg/ece/hlt/">HLT Lab</a>, National University of Singapore, Singapore.</li>
					<li>Sep 2014-Aug 2020, Ph.D. Student (Advisor: Prof. <a href="https://www.imu.edu.cn/info/1023/2690.htm">Guanglai Gao</a>)<br>
					Inner Mongolia University, Hohhot, China. <br>
					Research topic: Mongolian Text-to-Speech system <a href="http://mtts.mglip.com/"> [DEMO] </a>.</li>
					<li>Sep 2010-June 2014, Undergraduate<br>
					Taiyuan University of Technology, ShanXi, China. <br>
					</li>
					</ol>
					<br />
				</div>
			</div>
			<br>
			<h2>News <img src="new-icon.jpeg" width="100" height="50"></h2>
			<br>
			<div style="overflow-y: scroll; height:200px;">
			<UL>  

				
					<li>
			 <font color="red"> 2023/01 </font>
			  One paper about Speech Enhancement is accepted for publication in <font color="#1B58B8">IEEE/ACM-TASLP</font>.
			</li>
				

				<li>
			 <font color="red"> 2022/11 </font>
			 Two papers about Text-to-Speech has been accepted by <font color="#1B58B8">NCMMSC 2022</font>.

			</li>
				
		   <li>
			 <font color="red"> 2022/09 </font>
			 One paper about Open-Source Mongolian Text-to-Speech Synthesis Dataset has been accepted by <font color="#1B58B8">IALP 2022</font>.

			</li>



						<li>
			 <font color="red"> 2022/09 </font>
			 One paper about neural machine translation has been accepted by <font color="#1B58B8">ICONIP 2022</font>.

			</li>


						<li>
			 <font color="red"> 2022/09 </font>
			 Rui Liu's application for <font color="#1B58B8">Young Scientists Fund of the National Natural Science Foundation of China (NSFC) </font> was approved.

			</li>

				
					<li>
			 <font color="red"> 2022/06 </font>
			 Rui Liu was elected as member of Youth Working Committee of <font color="#1B58B8">Chinese Association for Artificial Intelligence (CAAI) </font>.
			</li>
				
				

				<li>
			 <font color="red"> 2022/06 </font>
			  One paper about emotional voice conversion is accepted for publication in <font color="#1B58B8">IEEE/ACM-TASLP</font>.
			</li>



			<li>
			 <font color="red"> 2022/06 </font>
			  One paper about emotion strength assessment has been accepted by <font color="#1B58B8">INTERSPEECH 2022</font>.
			</li>

				<li>
			 <font color="red">  2022/04 </font>One journal paper about Robust TTS is accepted for publication in <font color="#1B58B8">IEEE/ACM-TASLP</font>.
			</li>
				
				
					<li>
			 <font color="red">  2022/02 </font>
			  One journal paper about emotional TTS is accepted to be published in <font color="#1B58B8">IEEE Internet of Things Journal</font>.
			</li>
				
				<li>
			 <font color="red">  2022/01 </font>Two papers about Visual TTS and ASR have been accepted by <font color="#1B58B8">ICASSP 2022</font>. 
			</li>
				
				
				<li>
			 <font color="red">  2022/01 </font>
							 Rui Liu was elected as executive member of CCF Professional Committee of <font color="#1B58B8"><a href="https://www.ccf.org.cn/Chapters/TC/TC_Listing/TASDAP/" target="_blank">Speech Dialogue and Auditory Processing </a> </font>.
			</li>

		<li>
			 <font color="red">  2021/12 </font>
							One paper about Real-time and High-fidelity Mongolian TTS is accepted for publication in <font color="#1B58B8">Journal of Chinese Information Processing</font>.
			</li>

				
						<li>
			 <font color="red">  2021/12 </font>
							Our paper about Mongolian emotional speech synthesis was awarded as "<b><font color="red">Best Paper</font></b>" at <font color="#1B58B8"><a href="http://www.ialp2021.org/">IALP 2021</a></font>.
			</li>
				
			 
				 
					<li>
			 <font color="red">  2021/11 </font>
			  One journal paper about emotional voice conversion is accepted for publication in <font color="#1B58B8">Speech Communication</font>.
			</li>
				
				 
				
				<li>
			 <font color="red">  2021/10 </font>
			  Invited to serve as a reviewer for <font color="#1B58B8">ICASSP 2022</font>.
			</li>
				
				
	<li>
			 <font color="red">  2021/06 </font>
			  One paper about emotional TTS has been accepted by <font color="#1B58B8">INTERSPEECH 2021</font>.
			</li>


			<li>
			 <font color="red">  2021/04 </font>
			  One journal paper about expressive TTS is accepted for publication in <font color="#1B58B8">IEEE/ACM-TASLP</font>.
			</li>

			<li>
			 <font color="red">  2021/04 </font>
			  One journal paper about fast and high-quality TTS is accepted to be published in <font color="#1B58B8">Neural Networks</font>.
			</li>
				
			
				

			<li>
			 <font color="red"> 2021/01 </font>
			  Two papers about expressive TTS and emotional voice conversion  have been accepted by <font color="#1B58B8">ICASSP 2021</font>.
			</li>


		  	 <li>
			 <font color="red">  2020/12 </font>
			  One journal paper about expressive Mongolian TTS is accepted for publication in <font color="#1B58B8">IEEE/ACM-TASLP</font>.
			</li>
                
			</UL>
			</div>
			 
 			<br>
 
			 
		
			<h2 id="publication" class="page-header">Selected Publications   <a href="https://scholar.google.com.sg/citations?user=B2t0J-IAAAAJ&amp;hl=zh-CN" target="_blank" rel="external">[Google Scholar]</a></h2>
			(#: equal contribution *: corresponding author)
			<div class="row">
				<div class="col-md-12">
					<h4><b>Preprints</b></h4>
					<ol class="paper-list" id="grants">

					<li>
                    <b>Controllable Accented Text-to-Speech Synthesis </b> <br>
                    <b>Rui Liu</b>, Berrak Sisman, Guanglai Gao, Haizhou Li.<br>
                    <font color="#1B58B8">To be submitted for possible journal publication <br></font> 
                    <font color="#1B58B8"><a href="https://arxiv.org/abs/2209.10804" target="_blank">[PDF]</a> <a href="https://speechdemo.github.io/caitts/">[DEMO]</a></font>
                    </li>
                    
<li>
                    <b>Explicit Intensity Control for Accented Text-to-speech </b> <br>
                    <b>Rui Liu</b>, Haolin Zuo, De Hu, Guanglai Gao, Haizhou Li.<br>
                    <font color="#1B58B8">Submitted to ICASSP'2023 <br></font> 
                    <font color="#1B58B8"><a href="https://arxiv.org/abs/2210.15364" target="_blank">[PDF]</a> <a href="https://ttslr.github.io/Ai-TTS/">[DEMO]</a></font>
                    </li>


                    <li>
                    <b>FCTalker: Fine and Coarse Grained Context Modeling for Expressive Conversational Speech Synthesis </b> <br>
                    Yifan Hu, <b>Rui Liu <sup>*</sup></b>, Guanglai Gao, Haizhou Li.<br>
                    <font color="#1B58B8">Submitted to ICASSP'2023 <br></font> 
                    <font color="#1B58B8"><a href="https://arxiv.org/abs/2210.15360" target="_blank">[PDF]</a> <a href="https://walker-hyf.github.io/FCTalker/">[DEMO]</a></font> <a href="https://github.com/walker-hyf/FCTalker">[CODE]</a></font>
                    </li>



                    <li>
                    <b>Exploiting Modality-Invariant Feature for Robust Multimodal Emotion Recognition with Missing Modalities </b> <br>
                    Haolin Zuo, <b>Rui Liu <sup>*</sup></b>, Jinming Zhao, Guanglai Gao, Haizhou Li.<br>
                    <font color="#1B58B8">Submitted to ICASSP'2023 <br></font> 
                    <font color="#1B58B8"><a href="https://arxiv.org/abs/2210.15359" target="_blank">[PDF]</a> <a href="https://github.com/ZhuoYulang/IF-MMIN">[CODE]</a></font>
                    </li>



                       <li>
                    <b>Fast Subnetwork Selection for Speech Enhancement over Wireless Acoustic Sensor Networks</b> <br>
                    De Hu, Xu Wang, <b>Rui Liu</b>, Feilong Bao.<br>
                    <font color="#1B58B8">Submitted to ICASSP'2023 <br></font> 
                    <font color="#1B58B8"><a href="https://arxiv.org/abs/xxx" target="_blank">[PDF]</a> </font>
                    </li>




                       <li>
                    <b>Hybrid filtering for Real-Time Emotional Voice Conversion</b> <br>
                    Zhaojie Luo, <b>Rui Liu</b>, Sheng Li, Shuyun Tang, Jun Baba, Yuichiro Yoshikawa, Hiroshi Ishiguro<br>
                    <font color="#1B58B8">Submitted to ICASSP'2023 <br></font> 
                    <font color="#1B58B8"><a href="https://arxiv.org/abs/xxx" target="_blank">[PDF]</a> <a href="https://ZhaojieL.github.io/HF-RealtimeVC">[DEMO]</a></font>
                    </li>
					

				
				

						
						
					
						
						
 					</ol><br>

                    


					<h4><b>Journal papers</b></h4>
					<ol class="paper-list" id="grants">



					<li>
					<b>Decoupling Speaker-Independent Emotions for Voice Conversion Via Source-Filter Networks </b> <br>
					Zhaojie Luo, Shoufeng Lin, <b>Rui Liu <sup>*</sup></b>, Jun Baba, Yuichiro Yoshikawa, Ishiguro Hiroshi.<br>
					<font color="#1B58B8">IEEE/ACM Transactions on Audio, Speech, and Language Processing (IEEE/ACM-TASLP). 2022 <br></font><font color="red">(Top journal, JCR Q1, IF=4.364)</font><br>
					<font color="#1B58B8"><a href="https://ieeexplore.ieee.org/document/9829916" target="_blank">[PDF]</a> <a href="https://zhaojiel.github.io/SFEVC/">[DEMO]</a></font>
					</li>



					<li>
					<b>Decoding Knowledge Transfer for Neural Text-to-Speech Training</b> <br>
					<b>Rui Liu</b>, Berrak Sisman, Guanglai Gao, Haizhou Li.<br>
				  <font color="#1B58B8">IEEE/ACM Transactions on Audio, Speech, and Language Processing (IEEE/ACM-TASLP). 2022 <br></font><font color="red">(Top journal, JCR Q1, IF=4.364)</font>
					<br>
					<font color="#1B58B8"><a href="https://ieeexplore.ieee.org/document/9767637" target="_blank">[PDF]</a>  <a href="https://ttslr.github.io/MT-KD/">[DEMO]</a>  <a href="./papers/TASLP2022-decoding.txt" target="_blank">[BIB]</a></font>
					</li>



					<li>
					<b>Multi-Stage Deep Transfer Learning for EmIoT-enabled Human-Computer Interaction</b> <br>
					<b>Rui Liu</b>, Qi Liu, Hongxu Zhu, Hui Cao.<br>
					<font color="#1B58B8">IEEE Internet of Things Journal. 2022 </font><br> </font><font color="red">(Top journal, JCR Q1, IF=10.238)</font><br>
					<font color="#1B58B8"><a href="https://ieeexplore.ieee.org/document/9702532" target="_blank">[PDF]</a> <a href="https://ttslr.github.io/IOT/">[DEMO]</a></font>
					</li>	
						
					<li>
					<b>MonTTS: A Real-time and High-fidelity Mongolian TTS Model with Complete Non-autoregressive Mechanism (in Chinese) </b> <br>
					<b>Rui Liu</b>, Shyin Kang, Jingdong Li, Feilong Bao, Guanglai Gao.<br>
					<font color="#1B58B8">Journal of Chinese Information Processing. 2022 </font><br> </font><font color="red">(CCF T1)</font><br>
					<font color="#1B58B8"><a href="http://jcip.cipsc.org.cn/CN/abstract/abstract3357.shtml" target="_blank">[PDF]</a> <a href="https://github.com/ttslr/MonTTS">[CODE]</a></font>
					</li>
				
						
			                <li>
					<b>Emotional Voice Conversion: Theory, Databases and ESD </b> <br>
					Kun Zhou, Berrak Sisman, <b>Rui Liu</b>, Haizhou Li.<br>
					<font color="#1B58B8">Speech Communication. 2021 </font><br> </font><font color="red">(Top journal, CCF-B, IF=2.723)</font><br>
					<font color="#1B58B8"><a href="https://arxiv.org/abs/2105.14762" target="_blank">[PDF]</a> <a href="https://hltsingapore.github.io/ESD/demo.html">[DEMO]</a></font>
					</li>
						
						
  					<li>
					<b>Expressive TTS Training with Frame and Style Reconstruction Loss </b> <br>
					<b>Rui Liu</b>, Berrak Sisman, Guanglai Gao, Haizhou Li.<br>
					 <font color="#1B58B8">IEEE/ACM Transactions on Audio, Speech, and Language Processing (IEEE/ACM-TASLP). 2021 <br></font><font color="red">(Top journal, JCR Q1, IF=4.364)</font>
					<br>
					<font color="#1B58B8"><a href="https://ieeexplore.ieee.org/document/9420276" target="_blank">[PDF]</a>  <a href="https://ttslr.github.io/Expressive-TTS-Training-with-Frame-and-Style-Reconstruction-Loss/">[DEMO]</a>  <a href="./papers/TASLP2021-style.txt" target="_blank">[BIB]</a></font>
					</li>
					


					<li> 
					<b>FastTalker: A Neural Text-to-Speech Architecture with Shallow and Group Autoregression </b> <br>
					<b>Rui Liu</b>, Berrak Sisman, Yixing Lin, Haizhou Li.<br>
					<font color="#1B58B8">Neural Networks. 2021 </font>
					<br><font color="red">(JCR Q1, IF=9.657)</font>
					<br>
					<font color="#1B58B8"><a href="https://www.sciencedirect.com/science/article/pii/S0893608021001532" target="_blank">[PDF]</a> <a href="https://ttslr.github.io/FastTalker/">[DEMO]</a>  <a href="./papers/NN2021.txt" target="_blank">[BIB]</a> </font>
					</li>
					
          <li>
					<b>Exploiting Morphological and Phonological Features to Improve Prosodic Phrasing for Mongolian Speech Synthesis </b> <br>
					<b>Rui Liu</b>, Berrak Sisman, Feilong Bao, Jichen Yang, Guanglai Gao, Haizhou Li.<br>
					<font color="#1B58B8">IEEE/ACM Transactions on Audio, Speech, and Language Processing (IEEE/ACM-TASLP). 2021 <br></font><font color="red">(Top journal, JCR Q1, IF=4.364)</font>
					<br>
					<font color="#1B58B8"><a href="papers/TASLP2020Mongolian.pdf">[PDF]</a> <a href="./papers/TASLP2020Mongolian.txt" target="_blank">[BIB]</a></font>
					</li>
					
					<li>
					<b>Modeling Prosodic Phrasing with Multi-Task Learning in Tacotron-based TTS </b> <br>
					<b>Rui Liu</b>, Berrak Sisman, Feilong Bao, Guanglai Gao, Haizhou Li.<br>
					<font color="#1B58B8">IEEE Signal Processing Letters. 2020 </font>
					<br><font color="red">(JCR Q1, IF=3.201)</font>
					<br>
					<font color="#1B58B8"><a href="https://ieeexplore.ieee.org/document/9166626" target="_blank">[PDF]</a> <a href="./papers/SPL2020.txt" target="_blank">[BIB]</a> <a href="https://ttslr.github.io/SPL2020/">[DEMO]</a></font>
					</li>
					

					</ol>
				</div>
				<br />
				<br />
				<h6 style="color:white">f</h6>
				<div class="col-md-12">
					<h4><b>Conference papers</b></h4>
					<ol class="paper-list" id="grants">
					


					<li>
					<b>MnTTS: An Open-Source Mongolian Text-to-Speech Synthesis Dataset and Accompanied Baseline </b> <br>
					Yifan Hu <sup>#</sup>, Pengkai Yin <sup>#</sup>, <b>Rui Liu <sup>*</sup></b>, Feilong Bao and Guanglai Gao.<br>
					 <font color="#1B58B8">2022 International Conference on Asian Language Processing (IALP'2022) </font>   <br>
					<font color="#1B58B8"><a href="https://arxiv.org/abs/2209.10848" target="_blank">[PDF]</a> <a href="https://github.com/walker-hyf/MnTTS">[CODE]</a> <a href="http://mglip.com/corpus/corpus_detail.html?corpusid=20220819185345">[Application Entry]</a></font>
					</li>


					<li>
					<b>A Deep Investigation of RNN and Self-attention for the Cyrillic-Traditional Mongolian Bidirectional Conversion </b> <br>
					Muhan Na, <b>Rui Liu <sup>*</sup></b>, Feilong Bao and Guanglai Gao.<br>
					 <font color="#1B58B8">29th International Conference on Neural Information Processing (ICONIP'2022) </font> <br><font color="red">(CCF-C)</font> <br>
					<font color="#1B58B8"><a href="https://arxiv.org/abs/2209.11963" target="_blank">[PDF]</a> </font>
					</li>


					 <li>
					<b>Accurate Emotion Strength Assessment for Seen and Unseen Speech Based on Data-Driven Deep Learning </b> <br>
					<b>Rui Liu</b>, Berrak Sisman, Björn Schuller, Guanglai Gao and Haizhou Li.<br>
					 <font color="#1B58B8">23th Annual Conference of the International Speech Communication Association (INTERSPEECH'2022) </font> <br><font color="red">(Top conference, CCF-C)</font> <br>
					<font color="#1B58B8"><a href="https://www.isca-speech.org/archive/interspeech_2022/liu22i_interspeech.html" target="_blank">[PDF]</a> <a href="https://github.com/ttslr/StrengthNet">[CODE]</a></font>
					</li>



					<li>
					<b>Alignment-Learning based single-step decoding for accurate and fast non-autoregressive speech recognition</b> <br>
					Yonghe Wang, <b>Rui Liu <sup>*</sup></b>, Feilong Bao, Hui Zhang, Guanglai Gao.<br>
					<font color="#1B58B8">2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP'2022).</font> <br><font color="red">(Top conference, CCF-B)</font>
					<br>
					<font color="#1B58B8"><a href="https://ieeexplore.ieee.org/document/9746227" target="_blank">[PDF]</a> </font>
					</li>
						
						
					<li>
					<b>VisualTTS: TTS with Accurate Lip-speech Synchronization for Automatic Voice Over</b> <br>
					Junchen Lu, Berrak Sisman, <b>Rui Liu</b>, Mingyang Zhang, Haizhou Li.<br>
					<font color="#1B58B8">2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP'2022).</font> <br><font color="red">(Top conference, CCF-B)</font>
					<br>
					<font color="#1B58B8"><a href="https://arxiv.org/abs/2110.03342" target="_blank">[PDF]</a> <a href="https://ranacm.github.io/VisualTTS-Samples/">[DEMO]</a></font>
					</li>
						
						
					<li>
					<b>Mongolian emotional speech synthesis based on transfer learning and emotional embedding </b> <br>
					Aihong Huang, Feilong Bao, Guanglai Gao, Yu Shan, <b>Rui Liu <sup>*</sup></b><br>
						<font color="red"><b>(Best Paper Award)</b></font><br>
					<font color="#1B58B8">2021 International Conference on Asian Language Information Processing (IALP'2021) </font>  <br>
					<font color="#1B58B8"><a href="xx" target="_blank">[PDF]</a> </font>
					</li>
						
						
					<li>
					<b>Reinforcement Learning for Emotional Text-to-Speech Synthesis with Improved Emotion Discriminability </b> <br>
					<b>Rui Liu</b>, Berrak Sisman, Haizhou Li.<br>
					<font color="#1B58B8">22th Annual Conference of the International Speech Communication Association (INTERSPEECH'2021) </font> <br><font color="red">(Top conference, CCF-C)</font> <br>
					<font color="#1B58B8"><a href="https://www.isca-speech.org/archive/interspeech_2021/liu21p_interspeech.html" target="_blank">[PDF]</a> <a href="https://ttslr.github.io/i-ETTS/">[DEMO]</a>  <a href="./papers/InterSpeech2021.txt" target="_blank">[BIB]</a></font>
					</li>


 					<li>
					<b> GraphSpeech: Syntax-aware Graph Attention Network for Neural Speech Synthesis </b> <br>
					<b>Rui Liu</b>, Berrak Sisman, Haizhou Li.<br>
					<font color="#1B58B8">2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP'2021), Oral.</font> <br><font color="red">(Top conference, CCF-B)</font>
					<br>
					<font color="#1B58B8"><a href="https://arxiv.org/pdf/2010.12423.pdf" target="_blank">[PDF]</a> <a href="https://ttslr.github.io/GraphSpeech/">[DEMO]</a>  <a href="./papers/ICASSP2021.txt" target="_blank">[BIB]</a> <a href="https://www.bilibili.com/video/BV16U4y1t7cR/">[VIDEO]</a></font>
					</li>

					
					<li>
					<b>Seen and Unseen Emotional Style Transfer for Voice Conversion with a New Emotion Speech Dataset </b> <br>
					Kun Zhou, Berrak Sisman, <b>Rui Liu</b>, Haizhou Li.<br>
					<font color="#1B58B8">2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP'2021), Oral.</font> <br><font color="red">(Top conference, CCF-B)</font>
					<br>
					<font color="#1B58B8"><a href="https://arxiv.org/pdf/2010.14794.pdf" target="_blank">[PDF]</a> <a href="./papers/ICASSP2021-Kun.txt" target="_blank">[BIB]</a> </font>
					</li>
				    

					<li>
					<b>Teacher-Student Training For Robust Tacotron-based TTS </b> <br>
					<strong>Rui Liu</strong>, Berrak Sisman, Jingdong Li, Feilong Bao, Guanglai Gao, Haizhou Li.<br>
					<font color="#1B58B8">2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP'2020), Oral.  <br><font color="red">(Top conference, CCF-B) (With Travel Grant)</font></font><br>
					<font color="#1B58B8"><a href="papers/ICASSP2020.pdf">[PDF]</a> <a href="./papers/ICASSP2020.txt" target="_blank">[BIB]</a> <a href="https://ttslr.github.io/ICASSP2020/">[DEMO]</a> <a href="https://youtu.be/D6gTTuiDdPU">[VIDEO]</a></font>
					</li>

					<li>
					<b>WaveTTS: Tacotron-based TTS with Joint Time-Frequency Domain Loss </b> <br>
					<strong>Rui Liu</strong>, Berrak Sisman, Feilong Bao, Guanglai Gao, Haizhou Li.<br>
					<font color="#1B58B8">The Speaker and Language Recognition Workshop 2020 (Odyssey'2020).</font>
					<br>
					<font color="#1B58B8"><a href="papers/Odyssey2020.pdf">[PDF]</a> <a href="./papers/Odyssey2020.txt" target="_blank">[BIB]</a> <a href="https://ttslr.github.io/WaveTTS/">[DEMO]</a> <a href="xxx">[VIDEO]</a></font>
					</li>

					<li>
					<b>NUS-HLT System for Blizzard Challenge 2020 </b> <br>
					Yi Zhou, Xiaohai Tian, Xuehao Zhou, Mingyang Zhang, Grandee Lee, <strong>Rui Liu</strong>, Berrak Sisman, and Haizhou Li<br>
					<font color="#1B58B8">Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge 2020 (BC'2020).</font>
					<br> <font color="#1B58B8"><a href="papers/BC2020.pdf">[PDF]</a> <a href="./papers/BC2020.txt" target="_blank">[BIB]</a> </font>
					</li>


					<li>
					<b>The IMU Speech Synthesis Entry for Blizzard Challenge 2019 </b> <br>
					<strong>Rui Liu</strong>, Jingdong Li, Feilong Bao and Guanglai Gao.<br>
					<font color="#1B58B8">Blizzard Challenge Workshop 2019 (BC'2019).</font>
					<br>
					<font color="#1B58B8"><a href="papers/BC2019.pdf">[PDF]</a> <a href="./papers/BC2019.txt" target="_blank">[BIB]</a> </font>
					</li>

					<li>
					<b>Improving Mongolian Phrase Break Prediction by Using Syllable and Morphological Embeddings with BiLSTM Model </b> <br>
					<strong>Rui Liu</strong>, Feilong Bao, Guanglai Gao, Hui Zhang and Yonghe Wang.<br>
					<font color="#1B58B8">19th Annual Conference of the International Speech Communication Association (INTERSPEECH'2018), Oral </font>
					<br><font color="red">(Top conference, CCF-C)</font>
					<br>
					<font color="#1B58B8"><a href="https://www.isca-speech.org/archive_v0/Interspeech_2018/abstracts/1706.html">[PDF]</a> <a href="./papers/INTERSPEECH2018.txt" target="_blank">[BIB]</a> </font>
					</li>

					<li>
					<b>A LSTM Approach with Sub-word Embeddings for Mongolian Phrase Break Prediction </b> <br>
					<strong>Rui Liu</strong>, Feilong Bao, Guanglai Gao, Hui Zhang and Yonghe Wang.<br>
					<font color="#1B58B8">27th International Conference on Computational Linguistics (COLING'2018).</font>
					<br><font color="red">(Top conference, CCF-B)</font><br>
					<font color="#1B58B8"><a href="https://aclanthology.org/C18-1207/">[PDF]</a> <a href="./papers/COLING2018.txt" target="_blank">[BIB]</a> </font>
					</li>


					<li>
					<b>End-to-End Mongolian Text-to-Speech System </b> <br>
					Jingdong Li, Hui Zhang, <strong>Rui Liu</strong>, Xueliang Zhang and Feilong Bao.<br>
					<font color="#1B58B8">11th International Symposium on Chinese Spoken Language Processing (ISCSLP'2018).</font><br>
					<font color="#1B58B8"><a href="papers/ISCSLP2018.pdf">[PDF]</a> <a href="./papers/ISCSLP2018.txt" target="_blank">[BIB]</a></font>
					</li>


					<li>
					<b>Mongolian Text-to-Speech System Based on Deep Neural Network </b> <br>
					<strong>Rui Liu</strong>, Feilong Bao, Guanglai Gao and Yonghe Wang.<br>
					<font color="#1B58B8">14th National Conference on Man-Machine Speech Communication (NCMMSC'2017), Oral.</font>
					<br>
					<font color="#1B58B8"><a href="papers/NCMMSC2017.pdf">[PDF]</a> <a href="./papers/NCMMSC2017.txt" target="_blank">[BIB]</a> </font>
					</li>

					

					</ol>
				</div>
							
			</div>
				
			<h2 id="project" class="page-header">Projects</h2>
			<div class="row">
			<div class="col-md-12">
			<h4><b>Principal Investigator</b></h4>	
				<ol class="paper-list" id="grants">
					<li>
					High-level Talents Introduction Project of Inner Mongolia University
					<br> No. 10000-22311201/002
					<br> 2022/05-2025/05
					</li>
					<li>
					Young Scientists Fund of the National Natural Science Foundation of China (NSFC)
					<br> No. 62206136
					<br> 2023/01-2025/12
					</li>		
					</ol>
			<h4><b>Co-Principal Investigator</b></h4>	
				<ol class="paper-list" id="grants">
					<li>
					  .
					</li>
						
					</ol>
				</div>
			</div>



			<h2 id="talks" class="page-header">Talks</h2>
			<div class="row">
			<div class="col-md-12">
					<ol class="paper-list" id="grants">


						<li> 
					<b>Title: </b><u>Mongolian Text-to-Speech Technology</u> （蒙古语语音合成技术）. <br/>[<a href="./slides/多语种论坛-蒙古语TTS-v2.ppt" target="_blank">Slides</a>]
					[<a href="https://mp.weixin.qq.com/s/tdyJ0dygEwysYmGrfBFbxA" target="_blank">Video</a>]
<br/>
					<b>Organizer:</b> Chinese Association for Artificial Intelligence （CAAI） <br/>
					<b>Date:</b> 20 Aug 2022
					</li> 



					<li> 
					<b>Title: </b><u>Emotion Intensity Research of Speech Synthesis</u> (语音合成中的情感强度建模研究). <br/>[<a href="./slides/语音之家-情感强度建模-PPT.pdf" target="_blank">Slides</a>]
					[<a href="https://appzxw56sw27444.h5.xiaoeknow.com/v2/course/alive/l_6281eb95e4b0cedf38b26577?app_id=appzxw56sw27444&pro_id=&type=2&available=true&share_user_id=u_6278bea47e200_rUccGC7f14&share_type=5&scene=%E5%88%86%E4%BA%AB&is_redirect=1&share_scene=1&entry=2&entry_type=2002" target="_blank">Video</a>]
<br/>
					<b>Organizer:</b> SpeechHome （语音之家） <br/>
					<b>Date:</b> 19 May 2022
					</li> 


					<li>
					<b>Title: </b><u>Prosody and Emotion Modeling in End-to-End Speech Synthesis </u>（端到端语音合成中的韵律、情感建模研究）. <br/>[<a href="./slides/CCF专委会报告--语音合成韵律情感-刘瑞.pdf" target="_blank">Slides</a>]
					[<a href="https://mp.weixin.qq.com/s/O5ok2Uh0Sd639C5oqtRo8A" target="_blank">Video</a>]<br/>

					<b>Organizer:</b> CCF Professional Committee of Speech Dialogue and Auditory Processing <br/>
					<b>Date:</b> 04 Dec 2021
					</li>
					
									
					</ol>
				</div>
			</div>

			
			<h2 id="activities" class="page-header">Activities</h2>
			<div class="row">
				<div class="col-md-12">
					<ol class="paper-list" id="grants">



						<li><b>Conference Reviewer</b>:<br>
						- INTERSPEECH 2021/2022<br>
						- ICASSP 2021/2022<br>
						- Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge 2020<br>
						- SLT 2022<br>
						- O-COCOSDA 2022<br>

						<li><b>Journal Reviewer</b>: <br>
							- IEEE/ACM Transactions on Audio, Speech, and Language Processing (IEEE/ACM-TASLP) <br>
							- IEEE Signal Processing Letters<br>
							- IEEE Internet of Things Journal (IEEE-IoTJ)<br>

						<li><b>Professional Service</b>:
						<br>- Local Arrangement Co-chair, <a href="http://www.colips.org/conferences/cocosda2021/wp/" target="_blank">O-COCOSDA 2021</a>, Singapore.
						 <br>- Local Arrangement Co-chair, <a href="http://www.colips.org/conferences/iwsds2021/wp/" target="_blank">IWSDS 2021</a>, Singapore
						 <br>- Local Arrangement Co-chair, <a href="http://www.colips.org/conferences/sigdial2021/wp/" target="_blank">SIGDIAL 2021</a>, Singapore
						 <br>- Student Volunteer, ASRU 2019 (IEEE Automatic Speech Recognition and Understanding Workshop), Singapore
						 <br>- Student Volunteer, NLPCC 2018 (7th CCF International Conference on Natural Language Processing and Chinese Computing), Hohhot, China.
						 <br>- Program Committee Member, O-COCOSDA 2022.


					</ol>


				</div>
			</div>
			
			
			<h2 id="award" class="page-header">Awards</h2>
			<div class="row">
				<div class="col-md-12">
				<ol class="paper-list" id="grants">
					<li>Dec 2021, Excellent Doctoral dissertation of Inner Mongolia Autonomous Region</li>
					<li>Dec 2021, IALP-2021 <font color="red"><b>Best Paper</b></font></li>
					<li>July 2021,  <a href="https://www.acmturc.com/2021/en/doctoral_thesis_award.html" target="_blank"> 2020 ACM China Doctoral Dissertation Award </a> (Hohhot Chapter), ACM (Association for Computing Machinery) China Council </li>
					<li>Sep 2020, Excellent Doctoral dissertation of Inner Mongolia University</li>  
					<li>Feb 2020, ICASSP IEEE SPS Travel Grant </li>
						<li>Aug 2019,  Research Scholarship of China Scholarship Council (CSC) </li>
						<li>Oct 2018, National scholarship for Doctoral students (top 2% students), Ministry of Education of P.R.China  </li>
						<li>Oct 2018,  Academic scholarship of Inner Mongolia autonomous region </li>
						<li>Oct 2017,  National scholarship for Doctoral students (top 2% students), Ministry of Education of P.R.China </li>
						<li>Oct 2017, Academic scholarship of Inner Mongolia autonomous region </li>
						<li>Oct 2016,   Academic scholarship of Inner Mongolia autonomous region  </li>
						<li>Oct 2011, National Encouragement scholarship </li>
						 
					</ol>
				</div>
			</div>

			<h2 id="resource" class="page-header">Resource</h2>
			<div class="row">
				<div class="col-md-12">
				<ol class="resource-list" id="usefullinks">
					<li><a href="https://github.com/awesomedata/awesome-public-datasets" target="_blank"> Awesome-Public-Datasets</a></li>
					<li><a href="https://www.aminer.cn/ranks/conf" target="_blank"> Aminer Ranking</a></li>
					<li><a href="./resource/中国计算机学会推荐国际学术会议和期刊目录-2019.pdf" target="_blank"> CCF Ranking</a></li>
					<li><a href="http://www.jdl.ac.cn/how_to_research/index1_1.htm#1" target="_blank"> How to do research</a></li>
					<li><a href="https://github.com/bighuang624/AI-research-tools" target="_blank">AI-research-tools</a></li>
					<li><a href="https://numbda.cs.tsinghua.edu.cn/~yuwj/TH-CPL.pdf" target="_blank">TH-CPL</a>
					</li>
					<li><a href="https://mp.weixin.qq.com/s/Hu_ozQG_uoYDLN0jQzUhDQ" target="_blank">CCF Journal Ranking</a></li>
					<li><a href="https://ccfddl.github.io/" target="_blank">CCF Deadlines</a></li>
					
					
					    
					</ol>
				</div>
		     </div>
				<div align="center" style="margin:auto;padding-top:10px">
     

			</div>
		</div>
	</div>
</div>
<br><br><br>

 
<!-- <script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5pwqlxacb7n&amp;m=2&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script>
   -->
<div align="center" style="margin:auto;padding-top:10px">
            <div style="width:20%">
                <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=KVk19NwOIwG9DK1-a5YVH0cFfMaRh3fJ4gJF3U3zhmA&cl=ffffff&w=a"></script>
            </div>
        </div>
<br><br>



<!-- <img src="https://visitor-badge.glitch.me/badge?page_id=ttslr.github.io" alt="visitor">
<br>

<a href="https://www.revolvermaps.com/livestats/5j1361kk5zs/"><img src="//rf.revolvermaps.com/h/m/a/0/ff0000/128/0/5j1361kk5zs.png" width="256" height="128" alt="Map" style="border:0;"></a>


<a href="https://m.maploco.com/details/750608vu"><img style="border:0px;" src="https://www.maploco.com/vmap/s/10089210.png" alt="Locations of Site Visitors" title="Locations of Site Visitors"/></a>  


<script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=575nvmj2nw9&amp;m=7&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80" async="async"></script>
!-->

<footer class="footer">

	<div class="container">
		<p class="text-muted">Last updated on June, 2022 by Rui Liu.</p>
	</div>

	<div class="color_wrapper"></div>
</footer>
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    
    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="js/ie10-viewport-bug-workaround.js"></script>
  </body>

</html>
